{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GgFFArrrfBl"
      },
      "source": [
        "# Ler e gravar dados a partir de bancos de dados relacionais e não-relacionais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLmpWms1kZ5u",
        "outputId": "ed6e0e5b-3088-452d-efcd-1dc29d333600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting pyspark\n",
            "  Using cached pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
            "     command: /Library/Developer/CommandLineTools/usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/yz/810_n3xn7l55cl7k9168k3t40000gp/T/pip-install-bsijk8_e/pyspark_175f69749c474f65a8150dbb46a523d6/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/yz/810_n3xn7l55cl7k9168k3t40000gp/T/pip-install-bsijk8_e/pyspark_175f69749c474f65a8150dbb46a523d6/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/yz/810_n3xn7l55cl7k9168k3t40000gp/T/pip-pip-egg-info-t8o4n7ox\n",
            "         cwd: /private/var/folders/yz/810_n3xn7l55cl7k9168k3t40000gp/T/pip-install-bsijk8_e/pyspark_175f69749c474f65a8150dbb46a523d6/\n",
            "    Complete output (5 lines):\n",
            "    Traceback (most recent call last):\n",
            "      File \"<string>\", line 1, in <module>\n",
            "      File \"/private/var/folders/yz/810_n3xn7l55cl7k9168k3t40000gp/T/pip-install-bsijk8_e/pyspark_175f69749c474f65a8150dbb46a523d6/setup.py\", line 196, in <module>\n",
            "        with open('README.md') as f:\n",
            "    FileNotFoundError: [Errno 2] No such file or directory: 'README.md'\n",
            "    ----------------------------------------\u001b[0m\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/6d/08/87b404b8b3255d46caf0ecdccf871d501a2b58da9b844d3f9710ce9d4d53/pyspark-3.3.2.tar.gz#sha256=0dfd5db4300c1f6cc9c16d8dbdfb82d881b4b172984da71344ede1a9d4893da8 (from https://pypi.org/simple/pyspark/) (requires-python:>=3.7). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 210 kB/s eta 0:00:015\n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=f0528fda9b4258606fcb4184707ebd0443b345ea39501c8abe7a2c2f2434d58b\n",
            "  Stored in directory: /Users/Higor/Library/Caches/pip/wheels/51/c8/18/298a4ced8ebb3ab8a7d26a7198c0cc7035abb906bde94a4c4b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYw1KiQrvfkg",
        "outputId": "0207cc21-2522-4676-9e44-332c855530bf"
      },
      "outputs": [],
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.24.jar -P /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBAbcfLneyYs"
      },
      "source": [
        "### Conectar com o banco PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cs864bcvkho3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "siNGXTIQvqQi",
        "outputId": "abba3a2a-d374-4d1b-d11d-7f8f5c656d9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/04/09 03:35:47 WARN Utils: Your hostname, higor.local resolves to a loopback address: 127.0.0.1; using 192.168.0.7 instead (on interface en0)\n",
            "23/04/09 03:35:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/04/09 03:35:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"app_name\") \\\n",
        "    .config(\"spark.driver.extraClassPath\", \"jdbc_drivers\\postgresql-42.2.24.jar\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lNqqZqZRwc60"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"jdbc\") \\\n",
        "             .option(\"url\", \"jdbc:postgresql://postgres-database.chcgqj6jrjf8.us-west-2.rds.amazonaws.com/my_database\") \\\n",
        "             .option(\"dbtable\", \"vendas\") \\\n",
        "             .option(\"user\", \"postgres\") \\\n",
        "             .option(\"password\", \"123456789\") \\\n",
        "             .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "             .load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YETGFtv-ez1B"
      },
      "source": [
        "### Enviar a tabela para o banco PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teOnTPPVe0z2"
      },
      "outputs": [],
      "source": [
        "df.write.format(\"jdbc\") \\\n",
        "             .option(\"url\", \"jdbc:postgresql://postgres-database.chcgqj6jrjf8.us-west-2.rds.amazonaws.com/my_database\") \\\n",
        "             .option(\"dbtable\", \"df\") \\\n",
        "             .option(\"user\", \"postgres\") \\\n",
        "             .option(\"password\", \"123456789\") \\\n",
        "             .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "             .save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saqAnq9WugEK"
      },
      "source": [
        "### Conectar com o banco MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "MHmWoNFdul1k",
        "outputId": "290d1cbb-2b82-49e4-f797-667f5c41d5a1"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Leitura do MongoDB\") \\\n",
        "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "posts = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
        "    .option(\"database\", \"posts\") \\\n",
        "    .option(\"collection\", \"post\") \\\n",
        "    .load()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enviar dados para o MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "posts.write.format('mongo') \\\n",
        "    .option('uri', 'mongodb://localhost:27017/posts2.post') \\\n",
        "    .save()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
